{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_tf_keras_Compact_Convolutional_Transformers_my",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8i9JI0cyAN"
      },
      "source": [
        "# Compact Convolutional Transformers\n",
        "By example [Keras.io](https://keras.io/examples/vision/cct/)\n",
        "\n",
        "**Author:** [Sayak Paul](https://twitter.com/RisingSayak)<br>\n",
        "**Date created:** 2021/06/30<br>\n",
        "**Last modified:** 2021/06/30<br>\n",
        "**Description:** Compact Convolutional Transformers for efficient image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NVUcSeMcyAR"
      },
      "source": [
        "As discussed in the [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) paper,\n",
        "a Transformer-based architecture for vision typically requires a larger dataset than\n",
        "usual, as well as a longer pre-training schedule. [ImageNet-1k](http://imagenet.org/)\n",
        "(which has about a million images) is considered to fall under the medium-sized data regime with\n",
        "respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a typical\n",
        "Transformer-based architecture) do not have well-informed inductive biases (such as\n",
        "convolutions for processing images). This begs the question: can't we combine the\n",
        "benefits of convolution and the benefits of Transformers\n",
        "in a single network architecture? These benefits include parameter-efficiency, and\n",
        "self-attention to process long-range and global dependencies (interactions between\n",
        "different regions in an image).\n",
        "\n",
        "In [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704),\n",
        "Hassani et al. present an approach for doing exactly this. They proposed the\n",
        "**Compact Convolutional Transformer** (CCT) architecture. In this example, we will work on an\n",
        "implementation of CCT and we will see how well it performs on the CIFAR-10 dataset.\n",
        "\n",
        "If you are unfamiliar with the concept of self-attention or Transformers, you can read\n",
        "[this chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)\n",
        "from  François Chollet's book *Deep Learning with Python*. This example uses\n",
        "code snippets from another example,\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
        "\n",
        "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons, which can\n",
        "be installed using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ubRoYmcyAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63501417-5d41-42dd-d042-687517e69738"
      },
      "source": [
        "!pip install -U -q tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 12.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJL-I1GscyAT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTIW7EYucyAU"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqwCB7qAcyAU"
      },
      "source": [
        "## Hyperparameters and constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unt2PWUacyAV"
      },
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ApxtW-IcyAV"
      },
      "source": [
        "## Load CIFAR-10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VETmJ5cyAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6c4957-7da2-4131-dbf8-98ba83c9b6b6"
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQQ99e-icyAW"
      },
      "source": [
        "## The CCT tokenizer\n",
        "\n",
        "The first recipe introduced by the CCT authors is the tokenizer for processing the\n",
        "images. In a standard ViT, images are organized into uniform *non-overlapping* patches.\n",
        "This eliminates the boundary-level information present in between different patches. This\n",
        "is important for a neural network to effectively exploit the locality information. The\n",
        "figure below presents an illustration of how images are organized into patches.\n",
        "\n",
        "![](https://i.imgur.com/IkBK9oY.png)\n",
        "\n",
        "We already know that convolutions are quite good at exploiting locality information. So,\n",
        "based on this, the authors introduce an all-convolution mini-network to produce image\n",
        "patches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXPEdoMcyAX"
      },
      "source": [
        "\n",
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # This is our tokenizer.\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # Positional embeddings are optional in CCT. Here, we calculate\n",
        "        # the number of sequences and initialize an `Embedding` layer to\n",
        "        # compute the positional embeddings later.\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4qyfCCkcyAY"
      },
      "source": [
        "## Stochastic depth for regularization\n",
        "\n",
        "[Stochastic depth](https://arxiv.org/abs/1603.09382) is a regularization technique that\n",
        "randomly drops a set of layers. During inference, the layers are kept as they are. It is\n",
        "very much similar to [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) but only\n",
        "that it operates on a block of layers rather than individual nodes present inside a\n",
        "layer. In CCT, stochastic depth is used just before the residual blocks of a Transformers\n",
        "encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYOnVEvpcyAY"
      },
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuHico_ycyAZ"
      },
      "source": [
        "## MLP for the Transformers encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTY2119cyAZ"
      },
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQxUdnNcyAa"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "In the [original paper](https://arxiv.org/abs/2104.05704), the authors use\n",
        "[AutoAugment](https://arxiv.org/abs/1805.09501) to induce stronger regularization. For\n",
        "this example, we will be using the standard geometric augmentations like random cropping\n",
        "and flipping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vu0TxvRcyAa"
      },
      "source": [
        "# Note the rescaling layer. These layers have pre-defined inference behavior.\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1.0 / 255),\n",
        "        layers.RandomCrop(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ettJo0YccyAa"
      },
      "source": [
        "## The final CCT model\n",
        "\n",
        "Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only\n",
        "the feature map corresponding to the class token is pooled and is then used for the\n",
        "subsequent classification task (or any other downstream task). In CCT, outputs from the\n",
        "Transformers encoder are weighted and then passed on to the final task-specific layer (in\n",
        "this example, we do classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62aJuenccyAb"
      },
      "source": [
        "\n",
        "def create_cct_model(\n",
        "    image_size=image_size,\n",
        "    input_shape=input_shape,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim=projection_dim,\n",
        "    transformer_units=transformer_units,\n",
        "):\n",
        "\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "\n",
        "    # Encode patches.\n",
        "    cct_tokenizer = CCTTokenizer()\n",
        "    encoded_patches = cct_tokenizer(augmented)\n",
        "\n",
        "    # Apply positional embedding.\n",
        "    if positional_emb:\n",
        "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        position_embeddings = pos_embed(positions)\n",
        "        encoded_patches += position_embeddings\n",
        "\n",
        "    # Calculate Stochastic Depth probabilities.\n",
        "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for i in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection 1.\n",
        "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        x3 = StochasticDepth(dpr[i])(x3)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Apply sequence pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
        "    weighted_representation = tf.matmul(\n",
        "        attention_weights, representation, transpose_a=True\n",
        "    )\n",
        "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
        "\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(weighted_representation)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzpFo8zYcyAb"
      },
      "source": [
        "## Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Oi1AFDcyAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b37a2d6c-d42e-4838-a796-05710ae3a6dc"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "cct_model = create_cct_model()\n",
        "history = run_experiment(cct_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "352/352 [==============================] - 34s 81ms/step - loss: 1.8493 - accuracy: 0.3624 - top-5-accuracy: 0.8515 - val_loss: 1.5533 - val_accuracy: 0.5102 - val_top-5-accuracy: 0.9380\n",
            "Epoch 2/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.5197 - accuracy: 0.5268 - top-5-accuracy: 0.9394 - val_loss: 1.4262 - val_accuracy: 0.5664 - val_top-5-accuracy: 0.9576\n",
            "Epoch 3/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.3926 - accuracy: 0.5904 - top-5-accuracy: 0.9575 - val_loss: 1.3309 - val_accuracy: 0.6116 - val_top-5-accuracy: 0.9662\n",
            "Epoch 4/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.3157 - accuracy: 0.6269 - top-5-accuracy: 0.9663 - val_loss: 1.3011 - val_accuracy: 0.6372 - val_top-5-accuracy: 0.9672\n",
            "Epoch 5/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.2606 - accuracy: 0.6546 - top-5-accuracy: 0.9694 - val_loss: 1.3098 - val_accuracy: 0.6380 - val_top-5-accuracy: 0.9684\n",
            "Epoch 6/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.2239 - accuracy: 0.6713 - top-5-accuracy: 0.9732 - val_loss: 1.2625 - val_accuracy: 0.6590 - val_top-5-accuracy: 0.9728\n",
            "Epoch 7/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.1888 - accuracy: 0.6902 - top-5-accuracy: 0.9752 - val_loss: 1.1605 - val_accuracy: 0.6982 - val_top-5-accuracy: 0.9762\n",
            "Epoch 8/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.1586 - accuracy: 0.7058 - top-5-accuracy: 0.9779 - val_loss: 1.1695 - val_accuracy: 0.6960 - val_top-5-accuracy: 0.9768\n",
            "Epoch 9/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.1360 - accuracy: 0.7131 - top-5-accuracy: 0.9799 - val_loss: 1.1711 - val_accuracy: 0.6924 - val_top-5-accuracy: 0.9780\n",
            "Epoch 10/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.1104 - accuracy: 0.7230 - top-5-accuracy: 0.9820 - val_loss: 1.1059 - val_accuracy: 0.7234 - val_top-5-accuracy: 0.9800\n",
            "Epoch 11/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.0990 - accuracy: 0.7288 - top-5-accuracy: 0.9832 - val_loss: 1.1542 - val_accuracy: 0.7102 - val_top-5-accuracy: 0.9756\n",
            "Epoch 12/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.0876 - accuracy: 0.7356 - top-5-accuracy: 0.9825 - val_loss: 1.0932 - val_accuracy: 0.7308 - val_top-5-accuracy: 0.9810\n",
            "Epoch 13/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.0693 - accuracy: 0.7455 - top-5-accuracy: 0.9835 - val_loss: 1.0810 - val_accuracy: 0.7446 - val_top-5-accuracy: 0.9798\n",
            "Epoch 14/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.0516 - accuracy: 0.7518 - top-5-accuracy: 0.9845 - val_loss: 1.0951 - val_accuracy: 0.7360 - val_top-5-accuracy: 0.9786\n",
            "Epoch 15/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.0357 - accuracy: 0.7601 - top-5-accuracy: 0.9858 - val_loss: 1.0625 - val_accuracy: 0.7516 - val_top-5-accuracy: 0.9830\n",
            "Epoch 16/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 1.0297 - accuracy: 0.7633 - top-5-accuracy: 0.9857 - val_loss: 1.0621 - val_accuracy: 0.7500 - val_top-5-accuracy: 0.9834\n",
            "Epoch 17/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.0232 - accuracy: 0.7666 - top-5-accuracy: 0.9864 - val_loss: 1.0891 - val_accuracy: 0.7418 - val_top-5-accuracy: 0.9798\n",
            "Epoch 18/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.0102 - accuracy: 0.7752 - top-5-accuracy: 0.9860 - val_loss: 1.0501 - val_accuracy: 0.7564 - val_top-5-accuracy: 0.9842\n",
            "Epoch 19/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 1.0001 - accuracy: 0.7776 - top-5-accuracy: 0.9873 - val_loss: 1.0287 - val_accuracy: 0.7618 - val_top-5-accuracy: 0.9858\n",
            "Epoch 20/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 0.9905 - accuracy: 0.7812 - top-5-accuracy: 0.9878 - val_loss: 1.0656 - val_accuracy: 0.7484 - val_top-5-accuracy: 0.9858\n",
            "Epoch 21/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 0.9785 - accuracy: 0.7871 - top-5-accuracy: 0.9891 - val_loss: 1.0633 - val_accuracy: 0.7474 - val_top-5-accuracy: 0.9854\n",
            "Epoch 22/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9789 - accuracy: 0.7869 - top-5-accuracy: 0.9887 - val_loss: 1.0190 - val_accuracy: 0.7682 - val_top-5-accuracy: 0.9866\n",
            "Epoch 23/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9602 - accuracy: 0.7948 - top-5-accuracy: 0.9896 - val_loss: 1.0291 - val_accuracy: 0.7642 - val_top-5-accuracy: 0.9852\n",
            "Epoch 24/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9604 - accuracy: 0.7959 - top-5-accuracy: 0.9895 - val_loss: 1.0332 - val_accuracy: 0.7656 - val_top-5-accuracy: 0.9842\n",
            "Epoch 25/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9525 - accuracy: 0.7976 - top-5-accuracy: 0.9902 - val_loss: 1.0134 - val_accuracy: 0.7728 - val_top-5-accuracy: 0.9856\n",
            "Epoch 26/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 0.9501 - accuracy: 0.7989 - top-5-accuracy: 0.9893 - val_loss: 1.0225 - val_accuracy: 0.7712 - val_top-5-accuracy: 0.9878\n",
            "Epoch 27/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9413 - accuracy: 0.8047 - top-5-accuracy: 0.9907 - val_loss: 1.0251 - val_accuracy: 0.7690 - val_top-5-accuracy: 0.9852\n",
            "Epoch 28/30\n",
            "352/352 [==============================] - 27s 78ms/step - loss: 0.9380 - accuracy: 0.8074 - top-5-accuracy: 0.9899 - val_loss: 1.0166 - val_accuracy: 0.7800 - val_top-5-accuracy: 0.9852\n",
            "Epoch 29/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 0.9390 - accuracy: 0.8043 - top-5-accuracy: 0.9910 - val_loss: 1.0133 - val_accuracy: 0.7714 - val_top-5-accuracy: 0.9856\n",
            "Epoch 30/30\n",
            "352/352 [==============================] - 27s 77ms/step - loss: 0.9259 - accuracy: 0.8127 - top-5-accuracy: 0.9912 - val_loss: 1.0073 - val_accuracy: 0.7848 - val_top-5-accuracy: 0.9856\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 1.0371 - accuracy: 0.7733 - top-5-accuracy: 0.9835\n",
            "Test accuracy: 77.33%\n",
            "Test top 5 accuracy: 98.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBI4Ps67cyAc"
      },
      "source": [
        "Let's now visualize the training progress of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI0Cxzw0cyAc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "11dfc5c8-8b2c-4918-d696-2c0961a6fb7a"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "payfWt66cyAc"
      },
      "source": [
        "The CCT model we just trained has just **0.4 million** parameters, and it gets us to\n",
        "~78% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n",
        "well. This means we can train this network for longer (perhaps with a bit more\n",
        "regularization) and may obtain even better performance. This performance can further be\n",
        "improved by additional recipes like cosine decay learning rate schedule, other data augmentation\n",
        "techniques like [AutoAugment](https://arxiv.org/abs/1805.09501),\n",
        "[MixUp](https://arxiv.org/abs/1710.09412) or\n",
        "[Cutmix](https://arxiv.org/abs/1905.04899). With these modifications, the authors present\n",
        "95.1% top-1 accuracy on the CIFAR-10 dataset. The authors also present a number of\n",
        "experiments to study how the number of convolution blocks, Transformers layers, etc.\n",
        "affect the final performance of CCTs.\n",
        "\n",
        "For a comparison, a ViT model takes about **4.7 million** parameters and **100\n",
        "epochs** of training to reach a top-1 accuracy of 78.22% on the CIFAR-10 dataset. You can\n",
        "refer to\n",
        "[this notebook](https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb)\n",
        "to know about the experimental setup.\n",
        "\n",
        "The authors also demonstrate the performance of Compact Convolutional Transformers on\n",
        "NLP tasks and they report competitive results there."
      ]
    }
  ]
}